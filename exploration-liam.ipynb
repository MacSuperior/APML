{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1 - Outlier Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Due: Friday, 1 December, 2023 at 14:00 CET*\n",
    "\n",
    "For the first assignment of the course Applications of Machine Learning (INFOB3APML), you will learn to use decision tree, random forest, and isolation forest to detect an outlier class. The objectives of this assignment are:\n",
    "- use the supervised classification algorithms to classify outliers in real-life data sets\n",
    "- perform cross validation and fine-tune the model parameters of each algorithm\n",
    "- use the unsupervised outlier detection algorithms to detect outliers in real-life data sets\n",
    "- calculate model performance (e.g., accuracy, recall, precision, f1)\n",
    "- design experiments to compare performance of algorithms\n",
    "- reflect on the difference between different models\n",
    "\n",
    "\n",
    "This assignment includes three algorithms: DT, RF, and IF. The first task is to perform data exploration. In Task 2-4, you will use the three algorithms to classify outliers, respectively. In Task 5, you will compare the algorithms and evaluate their results. Please note that Task 2-4 have the following structure:\n",
    "1. First, find the library (e.g., sklearn examples) and try out the algorithm by simply training the model on the training data (do not consider any parameters or cross validation just yet); \n",
    "2. Train the model with the training data by using cross validation and find the best parameter setting for the parameters of interest;\n",
    "3. Report the average validation accuracy, recall, precision, and F1 scores of all validation sets;\n",
    "4. Finally, test the optimal model that has the best fitting parameters on your (held-out) test data, and report the test accuracy, precision, recall, and F1. \n",
    "\n",
    "Note that, in Task 5, you will need all the calculated accuracy, precision, recall and F1 measures from previous tasks. Make sure you save these to a list or dictionary so you can easily evaluate and compare the results. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Task 1: Exploring the data set\n",
    " \n",
    " \n",
    "\n",
    "### Data set: Bank Marketing\n",
    "\n",
    "\n",
    "Import the file *dataBank-additional-full_normalised.csv* to load the preprocessed data set. \"*The data is related with direct marketing campaigns of a Portuguese banking institution. The marketing campaigns were based on phone calls. Often, more than one contact to the same client was required, in order to access if the product (bank term deposit) would be ('yes') or not ('no') subscribed.*\"\n",
    "\n",
    "\n",
    "Use the column \"label\" as the response variable. The instances labeled with 1 are the \"outliers\", in this case the class we would like to detect accurately; the instance labeled with 0 are the inliers. \n",
    "\n",
    "\n",
    "The original data description can be found via the link here below. You will also find some explanations regarding the features under the section \"Attribute Information\".  \n",
    "https://archive.ics.uci.edu/ml/datasets/bank+marketing\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Import data\n",
    "data = pd.read_csv('./dataBank-additional-full_normalised_sampled.csv', sep=',')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 1.1. Exploratory data analysis\n",
    "\n",
    "For the data set, create 2-3 figures and tables that will help you understand the data. \n",
    "\n",
    "\n",
    "During the data exploration, you, as a team, are trying to get an impression about the data. You will create figures and/or tables that help you to get to know the data. While exploring the data, you may also consider answering the following questions, which may help you understand the data better. For example, \n",
    "\n",
    "- How many instances are there in each class? Are the classes imbalanced?\n",
    "- How many variables are in the data? What is the data type and the distribution of each variable? \n",
    "- Are the variables informative?\n",
    "- Are any pair of the potential predictor variables highly correlated?\n",
    "- (Should the variables be normalized or not?)\n",
    "- (Any relevant, useful preprocessing steps that may be taken?)\n",
    "\n",
    "#### Tips: \n",
    "\n",
    "Make sure to at least check the data type of each variable and to understand the distribution of each variable, especially the response variable. \n",
    "\n",
    "Try to find out what factors seem to determine whether an instance is an outlier or not. What do you conclude?\n",
    "\n",
    "*For creating data visualizations, you may consider using the matplot library and visit the [matplot gallery](https://matplotlib.org/stable/gallery/index.html) for inspiration (e.g., histograms for distribution, or heatmaps for feature correlation).*\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_classif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: plot figure(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Creating Train and Test data sets\n",
    "\n",
    "Create a training and a held-out test data set. *Later in Task 2-4, the training data will be used to perform cross-validation. The held-out test data will be used to evaluate the performance of the selected models.*\n",
    "\n",
    "Choose the size of your test data and motivate your choice when you discuss the experiment setup in your report. \n",
    "\n",
    "Tips: \n",
    "\n",
    "*You may use the [train_test_split](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) class provided by sklearn*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import method to make a train/test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# copy data\n",
    "df = data.copy()\n",
    "\n",
    "# create X and y\n",
    "features = ['age', 'job=housemaid', 'job=services', 'job=admin.', 'job=blue-collar',\n",
    "       'job=technician', 'job=retired', 'job=management', 'job=unemployed',\n",
    "       'job=self-employed', 'job=unknown', 'job=entrepreneur', 'job=student',\n",
    "       'marital=married', 'marital=single', 'marital=divorced',\n",
    "       'marital=unknown', 'education=basic.4y', 'education=high.school',\n",
    "       'education=basic.6y', 'education=basic.9y',\n",
    "       'education=professional.course', 'education=unknown',\n",
    "       'education=university.degree', 'education=illiterate', 'default=0',\n",
    "       'default=unknown', 'default=1', 'housing=0', 'housing=1',\n",
    "       'housing=unknown', 'loan=0', 'loan=1', 'loan=unknown',\n",
    "       'contact=cellular', 'month=may', 'month=jun', 'month=jul', 'month=aug',\n",
    "       'month=oct', 'month=nov', 'month=dec', 'month=mar', 'month=apr',\n",
    "       'month=sep', 'day_of_week=mon', 'day_of_week=tue', 'day_of_week=wed',\n",
    "       'day_of_week=thu', 'day_of_week=fri', 'duration', 'campaign', 'pdays',\n",
    "       'previous', 'poutcome=nonexistent', 'poutcome=failure',\n",
    "       'poutcome=success', 'emp.var.rate', 'cons.price.idx', 'cons.conf.idx',\n",
    "       'euribor3m', 'nr.employed']\n",
    "X = df[features]\n",
    "y = df['class']\n",
    "\n",
    "\n",
    "\n",
    "# TODO: create training data and held-out test data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_best = SelectKBest(f_classif)\n",
    "X_new = k_best.fit_transform(X, y)\n",
    "selected_features_indices = k_best.get_support(indices=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_feature_names = X.columns[selected_features_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.bar(x=selected_feature_names, y=k_best.scores_[selected_features_indices], title='Feature Importance Scores')\n",
    "fig.update_layout(xaxis_title='Features', yaxis_title='Importance Score')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Decision Trees \n",
    "### - Outlier Detection as a Supervised Classification\n",
    "\n",
    "### 2.1 Training a Decision Tree\n",
    "\n",
    "Use the basic [Decision Tree](http://scikit-learn.org/stable/modules/tree.html#tree) library in sklearn to learn a decision tree model by fitting the full training data.\n",
    "\n",
    "Show/plot the tree diagram and also plot the feature importances. \n",
    "What do you observe?\n",
    "\n",
    "\n",
    "#### Tips:\n",
    "\n",
    "To show the tree diagram, you may use the graphviz library or use the plot_tree function, see https://scikit-learn.org/stable/modules/tree.html\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learn a decision tree using default parameters\n",
    "cl = DecisionTreeClassifier()\n",
    "cl.fit(X_train, y_train)\n",
    "\n",
    "# plot the tree\n",
    "tree.plot_tree(cl)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: plot the feature importances\n",
    "importances = cl.feature_importances_\n",
    "feature_names = [i for i in range(X.shape[1])]\n",
    "tree_importances = pd.Series(importances, index=feature_names)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "tree_importances.plot.bar(ax=ax)\n",
    "ax.set_title(\"Feature importances using MDI\")\n",
    "ax.set_ylabel(\"Mean decrease in impurity\")\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.columns[[0, 50, 51, 59, 60, 61]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Confusion Matrix and Accuracy\n",
    "\n",
    "Compute the *confusion matrix* and *accuracy* of the tree using the held-out data set. Moreover, also compute the *recall*, *precision*, and *F1-score* of the tree. \n",
    "\n",
    "\n",
    "For this part, you can either implement your own functions or use the following scikit-learn libraries.  \n",
    "- [confusion matrix](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html#sklearn.metrics.confusion_matrix)\n",
    "- [accuracy score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html)\n",
    "- [recall](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.recall_score.html#sklearn.metrics.recall_score)\n",
    "- [precision](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_score.html#sklearn.metrics.precision_score)\n",
    "- [f1 score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html#sklearn.metrics.f1_score)\n",
    "- [classification report](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html#sklearn.metrics.classification_report)\n",
    "\n",
    "\n",
    "Reflect on the performance of the model and be aware of the difference between *accuracy* and *F1-score*. How good is this decision tree model for outlier detection?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the model to make predictions for the test data set\n",
    "y_pred = cl.predict(X_test)\n",
    "\n",
    "# TODO: compute accuracy, recall, precision, and f1 score\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "print(conf_matrix)\n",
    "print(f\"Accuracy: {accuracy}\\nRecall: {recall}\\nPrecision:{precision}\\nF1: {f1}\")\n",
    "print(report)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Features to Tree  (optional)\n",
    "\n",
    "Use the training data to re-fit a new decision tree with the parameter max_depth set to 4. Show the tree diagram and also plot the feature importances. \n",
    "\n",
    "Recalculate the performance of this simpler model. \n",
    "\n",
    "What do you observe?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: learn a decision tree with maximal depth 4\n",
    "d4_tree = DecisionTreeClassifier(max_depth=4)\n",
    "d4_tree.fit(X_train, y_train)\n",
    "\n",
    "# TODO: plot the tree\n",
    "tree.plot_tree(d4_tree)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the model to make predictions for the test data set\n",
    "y_pred_d4 = d4_tree.predict(X_test)\n",
    "\n",
    "# TODO: compute accuracy, recall, precision, and f1 score\n",
    "accuracy = accuracy_score(y_test, y_pred_d4)\n",
    "recall = recall_score(y_test, y_pred_d4)\n",
    "precision = precision_score(y_test, y_pred_d4)\n",
    "f1 = f1_score(y_test, y_pred_d4)\n",
    "report = classification_report(y_test, y_pred_d4)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred_d4)\n",
    "\n",
    "print(conf_matrix)\n",
    "print(f\"Accuracy: {accuracy}\\nRecall: {recall}\\nPrecision:{precision}\\nF1: {f1}\")\n",
    "print(report)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Cross validation (optional)\n",
    "\n",
    "The code example shown here below uses the [kfold](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html#sklearn.model_selection.KFold) method to implement 5-fold cross-validation. Moreover, it uses the cross validation to explore how the max_depth influences the model performance. It keeps track of the validation accuracy scores and F1-scores across the 5 folds. \n",
    "\n",
    "\n",
    "Now, change the code to also compute the recall and precision. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# create 5-fold cross-validation\n",
    "nk = 5\n",
    "kf = KFold(n_splits=nk, random_state=0, shuffle=True)\n",
    "\n",
    "# Search the parameter among the following\n",
    "C = np.arange(2, 10,)\n",
    "\n",
    "\n",
    "# init acc\n",
    "acc = np.zeros((nk , 8))\n",
    "# init f1\n",
    "f1 = np.zeros((nk , 8))\n",
    "i = 0\n",
    "for train_index , val_index in kf.split(X_train):\n",
    "    X_t, X_val = X_train.iloc[train_index], X_train.iloc[val_index]\n",
    "    y_t, y_val = y_train.iloc[train_index], y_train.iloc[val_index]\n",
    "    j = 0\n",
    "    for c in C:\n",
    "        dt = tree.DecisionTreeClassifier(min_samples_leaf = 1, max_depth = c)\n",
    "        dt.fit(X_t, y_t)\n",
    "        yhat = dt.predict(X_val)\n",
    "        acc[i][j] = accuracy_score(yhat , y_val)\n",
    "        f1[i][j] = f1_score(yhat , y_val)\n",
    "        j = j + 1\n",
    "    i = i + 1\n",
    "    \n",
    "print('Mean accuracy: ' + str(np.mean(acc , axis = 0)))\n",
    "print('Selected model index: ' + str(np.argmax(np.mean(acc , axis = 0))))\n",
    "\n",
    "print('Mean F1: ' + str(np.mean(f1 , axis = 0)))\n",
    "print('Selected model index: ' + str(np.argmax(np.mean(f1 , axis = 0))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "# Create 5-fold cross-validation\n",
    "nk = 5\n",
    "kf = KFold(n_splits=nk, random_state=0, shuffle=True)\n",
    "\n",
    "# Define the search space for parameters\n",
    "param_space = {'n_estimators': [10, 50, 100, 200], 'max_features': [None, 'sqrt', 'log2']}\n",
    "\n",
    "# Initialize accuracy and F1 arrays\n",
    "acc = np.zeros((nk, len(param_space['n_estimators']), len(param_space['max_features'])))\n",
    "f1 = np.zeros((nk, len(param_space['n_estimators']), len(param_space['max_features'])))\n",
    "\n",
    "i = 0\n",
    "for train_index, val_index in kf.split(X_train):\n",
    "    X_t, X_val = X_train.iloc[train_index], X_train.iloc[val_index]\n",
    "    y_t, y_val = y_train.iloc[train_index], y_train.iloc[val_index]\n",
    "\n",
    "    j = 0\n",
    "    for n_estimators in param_space['n_estimators']:\n",
    "        for max_features in param_space['max_features']:\n",
    "            # Create a Random Forest model\n",
    "            rf = RandomForestClassifier(n_estimators=n_estimators, max_features=max_features, random_state=0)\n",
    "            rf.fit(X_t, y_t)\n",
    "            yhat = rf.predict(X_val)\n",
    "\n",
    "            # Compute accuracy and F1 score\n",
    "            acc[i, j // len(param_space['max_features']), j % len(param_space['max_features'])] = accuracy_score(yhat, y_val)\n",
    "            f1[i, j // len(param_space['max_features']), j % len(param_space['max_features'])] = f1_score(yhat, y_val)\n",
    "\n",
    "            j += 1\n",
    "    i += 1\n",
    "\n",
    "# Compute mean accuracy and F1 score across folds\n",
    "mean_acc = np.mean(acc, axis=0)\n",
    "mean_f1 = np.mean(f1, axis=0)\n",
    "\n",
    "# Find the best parameters based on mean accuracy\n",
    "best_params_acc = np.unravel_index(np.argmax(mean_acc), mean_acc.shape)\n",
    "best_n_estimators_acc = param_space['n_estimators'][best_params_acc[0]]\n",
    "best_max_features_acc = param_space['max_features'][best_params_acc[1]]\n",
    "\n",
    "# Find the best parameters based on mean F1 score\n",
    "best_params_f1 = np.unravel_index(np.argmax(mean_f1), mean_f1.shape)\n",
    "best_n_estimators_f1 = param_space['n_estimators'][best_params_f1[0]]\n",
    "best_max_features_f1 = param_space['max_features'][best_params_f1[1]]\n",
    "\n",
    "# Train the best-performing model on the full training set\n",
    "best_rf_acc = RandomForestClassifier(n_estimators=best_n_estimators_acc, max_features=best_max_features_acc, random_state=0)\n",
    "best_rf_acc.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model on the held-out test data\n",
    "y_pred_test_acc = best_rf_acc.predict(X_test)\n",
    "acc_test = accuracy_score(y_pred_test_acc, y_test)\n",
    "f1_test = f1_score(y_pred_test_acc, y_test)\n",
    "\n",
    "print('Mean accuracy: ', mean_acc)\n",
    "print('Best parameters based on accuracy - n_estimators:', best_n_estimators_acc, ', max_features:', best_max_features_acc)\n",
    "print('Mean F1: ', mean_f1)\n",
    "print('Best parameters based on F1 score - n_estimators:', best_n_estimators_f1, ', max_features:', best_max_features_f1)\n",
    "print('Test accuracy of the best model: ', acc_test)\n",
    "print('Test F1 score of the best model: ', f1_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Tree Tuning\n",
    "\n",
    "\n",
    "\n",
    "The built-in decision tree algorithm you are using has several parameters which you can tune (e.g., *max_depth* and *min_samples_leaf*). Use 5-fold cross-validation (e.g., reuse the code of task 2.4 and adapt the code for two parameters), show how the choice of these parameters affects performance. \n",
    "\n",
    "\n",
    "#### Tips: \n",
    "Make a decision on the range of values that you would try for the two parameters and discuss your choice in the experiment setup section.\n",
    "\n",
    "Here is a guide that helps you to build the experiment.\n",
    "First, reuse the code of task 2.4 and show how max_depth affects train and **validation accuracy**. On a single axis, plot train and **validation accuracy** as a function of max_depth. Use a red line to show validation accuracy and a blue line to show train accuracy. (Do not use your (held-out) **test data** yet). \n",
    "\n",
    "Second, show how validation accuracy relates to both max_depth and min_samples_leaf. Specifically, create a 3-D plot where the x-axis is max_depth, the y-axis is min_samples_leaf, and the z-axis shows accuracy. What combination of max_depth and min-samples_leaf achieves the highest F1 score? How sensitive are the results to these two parameters? \n",
    "\n",
    "Finally, select the best-performing decision tree (i.e., the one that achieved the highest cross-validated performance) and report the performance of the fitted model on the held-out test data -- how does it compare to the cross-validated F1 score?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: create 5-fold cross-validation\n",
    "\n",
    "# TODO: set the search space of the parameters\n",
    "\n",
    "# TODO: learn an optimal decision tree model\n",
    "\n",
    "# TODO: create 2D (or 3D) plot that shows how the selected parameters affect the performance. \n",
    "\n",
    "# TODO: compute the performance of the model on your held-out test data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3. Random Forest\n",
    "### - Outlier Detection as a Supervised Classification\n",
    "\n",
    "Now use a [Random Forest](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html) to predict the labels for the data set. \n",
    "\n",
    "i) use the default values for the parameters to get a RF model running. \n",
    "\n",
    "ii) use 5-fold cross-validation to determine a possibly better choice for the parameter *n_estimators* and *max_features*\n",
    "    \n",
    "iii) select the best-performing decision tree (i.e., the one that achieved the highest cross-validated performance) and report the performance of the fitted model on the held-out test data ?\n",
    "\n",
    "In the report, reflect on how does the test performance of RF compare to the decision tree performance? \n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# TODO: create 5-fold cross-validation\n",
    "\n",
    "# TODO: set the search space of the parameters\n",
    "\n",
    "# TODO: learn an optimal random forest model\n",
    "\n",
    "# TODO: compute the performance of the model on your held-out test data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# i) Fit a Random Forest model with default parameters\n",
    "rf_default = RandomForestClassifier(random_state=42)\n",
    "rf_default.fit(X, y)\n",
    "\n",
    "# ii) Use 5-fold cross-validation to find the best values for n_estimators and max_features\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],  # Add more values as needed\n",
    "    'max_features': ['auto', 'sqrt', 'log2']  # Add more values as needed\n",
    "}\n",
    "\n",
    "rf_cv = RandomForestClassifier(random_state=42)\n",
    "grid_search = GridSearchCV(estimator=rf_cv, param_grid=param_grid, cv=5, scoring='accuracy')\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "# Get the best parameters from the grid search\n",
    "best_params = grid_search.best_params_\n",
    "\n",
    "# iii) Select the best-performing decision tree and report its performance on the test data\n",
    "best_rf = grid_search.best_estimator_\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the best Random Forest model on the training data\n",
    "best_rf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = best_rf.predict(X_test)\n",
    "\n",
    "# Report the accuracy of the best Random Forest model on the test set\n",
    "test_accuracy_rf = accuracy_score(y_test, y_pred)\n",
    "print(f\"Test Accuracy of the Best Random Forest Model: {test_accuracy_rf}\")\n",
    "\n",
    "# Compare with the performance of a single Decision Tree\n",
    "# Create and train a Decision Tree model\n",
    "dt = DecisionTreeClassifier(random_state=42)\n",
    "dt.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set using the Decision Tree model\n",
    "y_pred_dt = dt.predict(X_test)\n",
    "\n",
    "# Report the accuracy of the Decision Tree model on the test set\n",
    "test_accuracy_dt = accuracy_score(y_test, y_pred_dt)\n",
    "print(f\"Test Accuracy of the Decision Tree Model: {test_accuracy_dt}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Extract the results from the grid search\n",
    "cv_results = grid_search.cv_results_\n",
    "mean_cv_scores = np.array(cv_results['mean_test_score']).reshape(len(param_grid['n_estimators']), len(param_grid['max_features']))\n",
    "\n",
    "# Create a heatmap for cross-validation accuracy\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(mean_cv_scores, interpolation='nearest', cmap=plt.cm.YlGnBu)\n",
    "plt.title('Cross-validation accuracy')\n",
    "plt.xlabel('max_features')\n",
    "plt.ylabel('n_estimators')\n",
    "plt.colorbar()\n",
    "plt.xticks(np.arange(len(param_grid['max_features'])), param_grid['max_features'], rotation=45)\n",
    "plt.yticks(np.arange(len(param_grid['n_estimators'])), param_grid['n_estimators'])\n",
    "plt.tight_layout()\n",
    "\n",
    "# Create a line plot for test accuracy over time\n",
    "plt.subplot(1, 2, 2)\n",
    "n_estimators_values = [params['n_estimators'] for params in cv_results['params']]\n",
    "test_scores = cv_results['mean_test_score']\n",
    "\n",
    "plt.plot(n_estimators_values, test_scores, marker='o')\n",
    "plt.title('Test accuracy vs n_estimators')\n",
    "plt.xlabel('n_estimators')\n",
    "plt.ylabel('Test accuracy')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, KFold, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report, fbeta_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming X and y are your features and labels\n",
    "# X = ...\n",
    "# y = ...\n",
    "\n",
    "# Create a DataFrame to store the results\n",
    "results_df = pd.DataFrame(columns=['Split', 'Classifier', 'Accuracy', 'Precision', 'Recall', 'F1 Score', 'Confusion Matrix'])\n",
    "\n",
    "# Define the test sizes and number of folds\n",
    "test_sizes = [0.2, 0.3]\n",
    "k = 5\n",
    "\n",
    "# Define the parameter grid for grid search\n",
    "param_grid = {\n",
    "    'n_estimators': [10, 50, 100, 200],\n",
    "    'max_features': ['auto', 'sqrt', 'log2']\n",
    "}\n",
    "\n",
    "# Initialize the Random Forest classifier\n",
    "rf_classifier = RandomForestClassifier()\n",
    "\n",
    "# Perform grid search with cross-validation for different test sizes\n",
    "for test_size in test_sizes:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=5)\n",
    "\n",
    "    # Initialize GridSearchCV\n",
    "    grid_search = GridSearchCV(estimator=rf_classifier, param_grid=param_grid, cv=k, scoring='accuracy')\n",
    "    \n",
    "    # Fit the model\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    # Get the best parameters from the grid search\n",
    "    best_params = grid_search.best_params_\n",
    "\n",
    "    # Use the best parameters to train the Random Forest\n",
    "    rf_classifier = RandomForestClassifier(n_estimators=best_params['n_estimators'], max_features=best_params['max_features'])\n",
    "    rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "    # Predict on the test set\n",
    "    predictions = rf_classifier.predict(X_test)\n",
    "\n",
    "    # Calculate evaluation metrics for the test set\n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "    precision = precision_score(y_test, predictions)\n",
    "    recall = recall_score(y_test, predictions)\n",
    "    f1 = f1_score(y_test, predictions)\n",
    "    confusion = confusion_matrix(y_test, predictions)\n",
    "    class_report = classification_report(y_test, predictions)\n",
    "    fbeta = fbeta_score(y_test, predictions, beta=1.5)\n",
    "\n",
    "    # Store the results in the DataFrame\n",
    "    results_df = pd.concat([results_df, pd.Series({\n",
    "        'Split': f'Test Size {test_size}',\n",
    "        'Classifier': 'Random Forest',\n",
    "        'Accuracy': accuracy,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1 Score': f1,\n",
    "        'F-beta Score': fbeta\n",
    "    })], ignore_index=True)\n",
    "\n",
    "# Display or use the results as needed\n",
    "print(results_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4. Isolation Forest\n",
    "\n",
    "### 4.1 Apply Isolation Forest\n",
    "### - Outlier Detection as an Unsupervised Classification\n",
    "\n",
    "Use the [Isolation Forest Classifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.IsolationForest.html) to detect potential outliers in the data set. \n",
    "\n",
    "Select two parameters that you would like to investigate (for example, contamination, max_depth, n_estimators, max_samples). For each configuration: \n",
    "\n",
    "i) Apply Isolation Forest on the full data set (without using the labels Y)\n",
    "\n",
    "\n",
    "ii) Use the labels to compute the accuracy, recall, precision, and F1-score on the full data set (using the labels). \n",
    "\n",
    "\n",
    "Compare the performance of Isolation Forest of different configurations. \n",
    "\n",
    "\n",
    "#### Tips:\n",
    "\n",
    "- Note that the fit(X) function of the Isolation Forest does not use the labels. \n",
    "\n",
    "\n",
    "- **Look carefully at the values that an Isolation Forest classifier returns. Which value represents the outlier class? Be aware that you need to implement a mapping function f(x) that remaps -1 to 1 and 1 to 0, in order to transform the predictions such that the semantics are consistant with the previous classification algorithms.**\n",
    "\n",
    "\n",
    "- Create 2D (or 3D) plots to visualize your results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_parameter_values(results, parameter_name):\n",
    "    return [result['params'][parameter_name] for result in results]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(parameter_values, accuracy_values, parameter_name):\n",
    "    plt.scatter(parameter_values, accuracy_values)\n",
    "    plt.xlabel(parameter_name.capitalize())\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Isolation Forest Performance')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: set the search space of the parameters\n",
    "# TODO: apply the configured Isolation Forest model on the test set. \n",
    "# TODO: compute the performance of the model\n",
    "# TODO: return the optimal Isolation Forest model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.metrics import accuracy_score, recall_score, precaision_score, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "param_grid = {\n",
    "    'random_state': [42],\n",
    "    'contamination': [0.01, 0.05, 0.1, 0.15],\n",
    "    'max_samples': [50, 100, 200],\n",
    "    'max_features': [0.5, 0.7, 1.0],\n",
    "    'n_estimators': [50, 100, 200],\n",
    "}\n",
    "\n",
    "results = []\n",
    "\n",
    "for params in ParameterGrid(param_grid):\n",
    "    iforest = IsolationForest(**params)\n",
    "    iforest.fit(X_train)\n",
    "\n",
    "    predictions = iforest.predict(X_test)\n",
    "    mapped_predictions = [1 if p == -1 else 0 for p in predictions]\n",
    "\n",
    "    accuracy = accuracy_score(y_test, mapped_predictions)\n",
    "    recall = recall_score(y_test, mapped_predictions)\n",
    "    precision = precision_score(y_test, mapped_predictions)\n",
    "    f1 = f1_score(y_test, mapped_predictions)\n",
    "\n",
    "    results.append({\n",
    "        'params': params,\n",
    "        'accuracy': accuracy,\n",
    "        'recall': recall,\n",
    "        'precision': precision,\n",
    "        'f1': f1,\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for result in results:\n",
    "    print(\"Parameters:\", result['params'])\n",
    "    print(\"Accuracy:\", result['accuracy'])\n",
    "    print(\"Recall:\", result['recall'])\n",
    "    print(\"Precision:\", result['precision'])\n",
    "    print(\"F1 Score:\", result['f1'])\n",
    "    print(\"\\n\")\n",
    "\n",
    "\n",
    "contamination_values = get_parameter_values(results, 'contamination')\n",
    "max_samples_values = get_parameter_values(results, 'max_samples')\n",
    "n_estimators_values = get_parameter_values(results, 'n_estimators')\n",
    "max_features_values = get_parameter_values(results, 'max_features')\n",
    "accuracy_values = [result['accuracy'] for result in results]\n",
    "\n",
    "plot_results(contamination_values, accuracy_values, 'contamination')\n",
    "plot_results(max_samples_values, accuracy_values, 'max_samples')\n",
    "plot_results(n_estimators_values, accuracy_values, 'n_estimators')\n",
    "plot_results(max_features_values, accuracy_values, 'max_features')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can clearly see that all parameters except contamination have no correlaction with the accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Outlier Detection - Analyze Outliers\n",
    "\n",
    "Taking the best performing Isolation Forest model from Task 4.1, enrich the data set with the predicted labels (or scores) by the model. \n",
    "\n",
    "Perform one or two analyses to show the characteristics of the outliers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TODO: enrich the data with the anomaly scores assigned by the optimal model. \n",
    "\n",
    "\n",
    "# TODO: Perform one or two analyses to show the characteristics of the outliers. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enrich the data with the anomaly scores assigned by the optimal model. \n",
    "best_result = max(results, key=lambda x: x['f1'])\n",
    "best_params = best_result['params']\n",
    "\n",
    "best_iforest = IsolationForest(**best_params)\n",
    "best_iforest.fit(X_train)\n",
    "\n",
    "enriched_data = X_test.copy()\n",
    "enriched_data['isolation_forest_score'] = best_iforest.decision_function(X)\n",
    "\n",
    "# Perform one or two analyses to show the characteristics of the outliers. \n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(enriched_data.loc[y_test == 0, 'isolation_forest_score'], bins=50, label='Normal', alpha=0.5)\n",
    "plt.hist(enriched_data.loc[y_test == 1, 'isolation_forest_score'], bins=50, label='Outlier', alpha=0.5)\n",
    "plt.xlabel('Isolation Forest Score')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Isolation Forest Scores for Normal and Outlier Instances')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5. Report your results and discuss your findings\n",
    "\n",
    "### 5.1 Compare the performances \n",
    "\n",
    "By now, you have applied three algorithms with different parameters on the data set. For each algorithm, you have create tables or figures which you can add to your report. Discuss the results and their optimal performance. \n",
    "\n",
    "Create an overview table or figure that show the optimal performance of each algorithm on the data set, for example see the table here below. \n",
    "\n",
    "Discuss your findings in the report and reflect on the following questions:\n",
    "- According to the performance results, which one is the optimal model? \n",
    "- How large is the difference between the accuracy score and the F1 score for each model? What caused the difference?\n",
    "- Which of performance measures (the accuracy score, recall, precision, or F1-score) would you use for comparing the model performance? Why?\n",
    "- You are comparing the performance of supervised algorithms (DT and RF) with an unsupervised algorithm (Isolation Forest). Is this a fair comparison? Motivate your answer. \n",
    "\n",
    "\n",
    "\n",
    "| Model | Validation Accuracy  | Test Accuracy |  Validation Recall  |  Test Recall  | Validation F1 | Test F1 |... |\n",
    "|------|------|------|------|------|------|------|-----|\n",
    "|   Decision Tree        |  |  | | | | |\n",
    "|   Random Forest  |  |  | || | |\n",
    "|   Isolation Forest        |  |  | || | |\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Analyze and discuss the results\n",
    "\n",
    "For each optimal model, enrich your test set by adding the predicted labels by this model to the test set. Can you think of an analysis that gives insights into when the model performs poorly?\n",
    "\n",
    "Discuss the analysis and insights in the report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus Tasks \n",
    "\n",
    "We would like to challenge you with the following bonus tasks. For each task that is successfully completed, you may obtain max. 0.5 extra point. \n",
    "\n",
    "### Bonus Task 1\n",
    "\n",
    "Implement another outlier detection algorithm (for example, LOF, OC-SVM) or design your own outlier detection algorithm that achieves a better F1 score. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus Task 2 \n",
    "\n",
    "Implement techniques (e.g., preprocessing, feature engineering, sampling) that help improve the F1 scores of existing models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction Challenge \n",
    "\n",
    "- Import the independent test set without labels, apply your best performing model on this test set. \n",
    "\n",
    "- Enrich the test set with the predicted labels (**name this column 'predictedClass'**) \n",
    "\n",
    "- Export both the model as pkl file and the enriched test data set as a csv file. \n",
    "\n",
    "- The top three teams that have achieved the best accuracy score wins max. 0.3 bonus points.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# import data\n",
    "data_challenge = pd.read_csv('./dataBank-new_test_nolabel.csv', sep=',')\n",
    "X_new = data_challenge[features]\n",
    "print(X_new.describe())\n",
    "\n",
    "# TODO: assign optimal model \n",
    "optimal_model = ...\n",
    "\n",
    "yhat = optimal_model.predict(X_new)\n",
    "\n",
    "# TODO: enrich the data with the predicted labels by adding the column 'predictedClass'\n",
    "\n",
    "\n",
    "# TODO: export the enriched data to disk\n",
    "\n",
    "\n",
    "# export the model to disk\n",
    "modelfilename = 'Team_x_optimal_model.sav'\n",
    "pickle.dump(optimal_model, open(modelfilename, 'wb'))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
