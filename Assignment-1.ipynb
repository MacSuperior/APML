{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1 - Outlier Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Due: Friday, 1 December, 2023 at 14:00 CET*\n",
    "\n",
    "For the first assignment of the course Applications of Machine Learning (INFOB3APML), you will learn to use decision tree, random forest, and isolation forest to detect an outlier class. The objectives of this assignment are:\n",
    "- use the supervised classification algorithms to classify outliers in real-life data sets\n",
    "- perform cross validation and fine-tune the model parameters of each algorithm\n",
    "- use the unsupervised outlier detection algorithms to detect outliers in real-life data sets\n",
    "- calculate model performance (e.g., accuracy, recall, precision, f1)\n",
    "- design experiments to compare performance of algorithms\n",
    "- reflect on the difference between different models\n",
    "\n",
    "\n",
    "This assignment includes three algorithms: DT, RF, and IF. The first task is to perform data exploration. In Task 2-4, you will use the three algorithms to classify outliers, respectively. In Task 5, you will compare the algorithms and evaluate their results. Please note that Task 2-4 have the following structure:\n",
    "1. First, find the library (e.g., sklearn examples) and try out the algorithm by simply training the model on the training data (do not consider any parameters or cross validation just yet); \n",
    "2. Train the model with the training data by using cross validation and find the best parameter setting for the parameters of interest;\n",
    "3. Report the average validation accuracy, recall, precision, and F1 scores of all validation sets;\n",
    "4. Finally, test the optimal model that has the best fitting parameters on your (held-out) test data, and report the test accuracy, precision, recall, and F1. \n",
    "\n",
    "Note that, in Task 5, you will need all the calculated accuracy, precision, recall and F1 measures from previous tasks. Make sure you save these to a list or dictionary so you can easily evaluate and compare the results. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Task 1: Exploring the data set\n",
    " \n",
    " \n",
    "\n",
    "### Data set: Bank Marketing\n",
    "\n",
    "\n",
    "Import the file *dataBank-additional-full_normalised.csv* to load the preprocessed data set. \"*The data is related with direct marketing campaigns of a Portuguese banking institution. The marketing campaigns were based on phone calls. Often, more than one contact to the same client was required, in order to access if the product (bank term deposit) would be ('yes') or not ('no') subscribed.*\"\n",
    "\n",
    "\n",
    "Use the column \"label\" as the response variable. The instances labeled with 1 are the \"outliers\", in this case the class we would like to detect accurately; the instance labeled with 0 are the inliers. \n",
    "\n",
    "\n",
    "The original data description can be found via the link here below. You will also find some explanations regarding the features under the section \"Attribute Information\".  \n",
    "https://archive.ics.uci.edu/ml/datasets/bank+marketing\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Import data\n",
    "data = pd.read_csv('./dataBank-additional-full_normalised_sampled.csv', sep=',')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 1.1. Exploratory data analysis\n",
    "\n",
    "For the data set, create 2-3 figures and tables that will help you understand the data. \n",
    "\n",
    "\n",
    "During the data exploration, you, as a team, are trying to get an impression about the data. You will create figures and/or tables that help you to get to know the data. While exploring the data, you may also consider answering the following questions, which may help you understand the data better. For example, \n",
    "\n",
    "- How many instances are there in each class? Are the classes imbalanced?\n",
    "- How many variables are in the data? What is the data type and the distribution of each variable? \n",
    "- Are the variables informative?\n",
    "- Are any pair of the potential predictor variables highly correlated?\n",
    "- (Should the variables be normalized or not?)\n",
    "- (Any relevant, useful preprocessing steps that may be taken?)\n",
    "\n",
    "#### Tips: \n",
    "\n",
    "Make sure to at least check the data type of each variable and to understand the distribution of each variable, especially the response variable. \n",
    "\n",
    "Try to find out what factors seem to determine whether an instance is an outlier or not. What do you conclude?\n",
    "\n",
    "*For creating data visualizations, you may consider using the matplot library and visit the [matplot gallery](https://matplotlib.org/stable/gallery/index.html) for inspiration (e.g., histograms for distribution, or heatmaps for feature correlation).*\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_classif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: plot figure(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Creating Train and Test data sets\n",
    "\n",
    "Create a training and a held-out test data set. *Later in Task 2-4, the training data will be used to perform cross-validation. The held-out test data will be used to evaluate the performance of the selected models.*\n",
    "\n",
    "Choose the size of your test data and motivate your choice when you discuss the experiment setup in your report. \n",
    "\n",
    "Tips: \n",
    "\n",
    "*You may use the [train_test_split](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) class provided by sklearn*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import method to make a train/test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# copy data\n",
    "df = data.copy()\n",
    "\n",
    "# create X and y\n",
    "features = ['age', 'job=housemaid', 'job=services', 'job=admin.', 'job=blue-collar',\n",
    "       'job=technician', 'job=retired', 'job=management', 'job=unemployed',\n",
    "       'job=self-employed', 'job=unknown', 'job=entrepreneur', 'job=student',\n",
    "       'marital=married', 'marital=single', 'marital=divorced',\n",
    "       'marital=unknown', 'education=basic.4y', 'education=high.school',\n",
    "       'education=basic.6y', 'education=basic.9y',\n",
    "       'education=professional.course', 'education=unknown',\n",
    "       'education=university.degree', 'education=illiterate', 'default=0',\n",
    "       'default=unknown', 'default=1', 'housing=0', 'housing=1',\n",
    "       'housing=unknown', 'loan=0', 'loan=1', 'loan=unknown',\n",
    "       'contact=cellular', 'month=may', 'month=jun', 'month=jul', 'month=aug',\n",
    "       'month=oct', 'month=nov', 'month=dec', 'month=mar', 'month=apr',\n",
    "       'month=sep', 'day_of_week=mon', 'day_of_week=tue', 'day_of_week=wed',\n",
    "       'day_of_week=thu', 'day_of_week=fri', 'duration', 'campaign', 'pdays',\n",
    "       'previous', 'poutcome=nonexistent', 'poutcome=failure',\n",
    "       'poutcome=success', 'emp.var.rate', 'cons.price.idx', 'cons.conf.idx',\n",
    "       'euribor3m', 'nr.employed']\n",
    "X = df[features]\n",
    "y = df['class']\n",
    "\n",
    "# TODO: create training data and held-out test data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_best = SelectKBest(f_classif)\n",
    "X_new = k_best.fit_transform(X, y)\n",
    "selected_features_indices = k_best.get_support(indices=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_feature_names = X.columns[selected_features_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.bar(x=selected_feature_names, y=k_best.scores_[selected_features_indices], title='Feature Importance Scores')\n",
    "fig.update_layout(xaxis_title='Features', yaxis_title='Importance Score')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Decision Trees \n",
    "### - Outlier Detection as a Supervised Classification\n",
    "\n",
    "### 2.1 Training a Decision Tree\n",
    "\n",
    "Use the basic [Decision Tree](http://scikit-learn.org/stable/modules/tree.html#tree) library in sklearn to learn a decision tree model by fitting the full training data.\n",
    "\n",
    "Show/plot the tree diagram and also plot the feature importances. \n",
    "What do you observe?\n",
    "\n",
    "\n",
    "#### Tips:\n",
    "\n",
    "To show the tree diagram, you may use the graphviz library or use the plot_tree function, see https://scikit-learn.org/stable/modules/tree.html\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# learn a decision tree using default parameters\n",
    "cl = DecisionTreeClassifier()\n",
    "cl.fit(X_train, y_train)\n",
    "\n",
    "# plot the tree\n",
    "tree.plot_tree(cl)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: plot the feature importances\n",
    "importances = sorted(cl.feature_importances_, reverse=True)\n",
    "feature_names = [i for i in range(X.shape[1])]\n",
    "tree_importances = pd.Series(importances, index=feature_names)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "tree_importances.plot.bar(ax=ax)\n",
    "ax.set_title(\"Feature importances using MDI\")\n",
    "ax.set_ylabel(\"Mean decrease in impurity\")\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "importances = cl.feature_importances_\n",
    "indices = importances.argsort()[::-1]\n",
    "top_feature_names = X.columns[indices[:6]]\n",
    "top_importances = importances[indices[:6]]\n",
    "tree_importances = pd.Series(top_importances, index=top_feature_names)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "tree_importances.plot.bar(ax=ax)\n",
    "ax.set_title(\"Feature importances using MDI\")\n",
    "ax.set_ylabel(\"Mean decrease in impurity\")\n",
    "fig.tight_layout()\n",
    "\n",
    "print(\"Top 6 important features:\")\n",
    "print(top_feature_names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Confusion Matrix and Accuracy\n",
    "\n",
    "Compute the *confusion matrix* and *accuracy* of the tree using the held-out data set. Moreover, also compute the *recall*, *precision*, and *F1-score* of the tree. \n",
    "\n",
    "\n",
    "For this part, you can either implement your own functions or use the following scikit-learn libraries.  \n",
    "- [confusion matrix](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html#sklearn.metrics.confusion_matrix)\n",
    "- [accuracy score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html)\n",
    "- [recall](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.recall_score.html#sklearn.metrics.recall_score)\n",
    "- [precision](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_score.html#sklearn.metrics.precision_score)\n",
    "- [f1 score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html#sklearn.metrics.f1_score)\n",
    "- [classification report](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html#sklearn.metrics.classification_report)\n",
    "\n",
    "\n",
    "Reflect on the performance of the model and be aware of the difference between *accuracy* and *F1-score*. How good is this decision tree model for outlier detection?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "\n",
    "# use the model to make predictions for the test data set\n",
    "y_pred = cl.predict(X_test)\n",
    "\n",
    "# TODO: compute accuracy, recall, precision, and f1 score\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "print(conf_matrix)\n",
    "print(f\"Accuracy: {accuracy}\\nRecall: {recall}\\nPrecision:{precision}\\nF1: {f1}\")\n",
    "print(report)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Features to Tree  (optional)\n",
    "\n",
    "Use the training data to re-fit a new decision tree with the parameter max_depth set to 4. Show the tree diagram and also plot the feature importances. \n",
    "\n",
    "Recalculate the performance of this simpler model. \n",
    "\n",
    "What do you observe?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: learn a decision tree with maximal depth 4\n",
    "d4_tree = DecisionTreeClassifier(max_depth=4)\n",
    "d4_tree.fit(X_train, y_train)\n",
    "\n",
    "# TODO: plot the tree\n",
    "tree.plot_tree(d4_tree)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the model to make predictions for the test data set\n",
    "y_pred_d4 = d4_tree.predict(X_test)\n",
    "\n",
    "# TODO: compute accuracy, recall, precision, and f1 score\n",
    "accuracy = accuracy_score(y_test, y_pred_d4)\n",
    "recall = recall_score(y_test, y_pred_d4)\n",
    "precision = precision_score(y_test, y_pred_d4)\n",
    "f1 = f1_score(y_test, y_pred_d4)\n",
    "report = classification_report(y_test, y_pred_d4)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred_d4)\n",
    "\n",
    "print(conf_matrix)\n",
    "print(f\"Accuracy: {accuracy}\\nRecall: {recall}\\nPrecision:{precision}\\nF1: {f1}\")\n",
    "print(report)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Cross validation (optional)\n",
    "\n",
    "The code example shown here below uses the [kfold](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html#sklearn.model_selection.KFold) method to implement 5-fold cross-validation. Moreover, it uses the cross validation to explore how the max_depth influences the model performance. It keeps track of the validation accuracy scores and F1-scores across the 5 folds. \n",
    "\n",
    "\n",
    "Now, change the code to also compute the recall and precision. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# create 5-fold cross-validation\n",
    "nk = 5\n",
    "kf = KFold(n_splits=nk, random_state=0, shuffle=True)\n",
    "\n",
    "# Search the parameter among the following\n",
    "C = np.arange(2, 10,)\n",
    "\n",
    "\n",
    "# init acc\n",
    "acc = np.zeros((nk , 8))\n",
    "# init f1\n",
    "f1 = np.zeros((nk , 8))\n",
    "# init recall\n",
    "recall = np.zeros((nk , 8))\n",
    "# init precision\n",
    "precision = np.zeros((nk , 8))\n",
    "\n",
    "i = 0\n",
    "for train_index , val_index in kf.split(X_train):\n",
    "    X_t, X_val = X_train.iloc[train_index], X_train.iloc[val_index]\n",
    "    y_t, y_val = y_train.iloc[train_index], y_train.iloc[val_index]\n",
    "    j = 0\n",
    "    for c in C:\n",
    "        dt = tree.DecisionTreeClassifier(min_samples_leaf = 1, max_depth = c)\n",
    "        dt.fit(X_t, y_t)\n",
    "        yhat = dt.predict(X_val)\n",
    "        acc[i][j] = accuracy_score(yhat , y_val)\n",
    "        f1[i][j] = f1_score(yhat , y_val)\n",
    "        recall[i][j] = recall_score(yhat , y_val)\n",
    "        precision[i][j] = precision_score(yhat , y_val)\n",
    "        j = j + 1\n",
    "    i = i + 1\n",
    "    \n",
    "print('Mean accuracy: ' + str(np.mean(acc , axis = 0)))\n",
    "print('Selected model index: ' + str(np.argmax(np.mean(acc , axis = 0))))\n",
    "\n",
    "print('Mean F1: ' + str(np.mean(f1 , axis = 0)))\n",
    "print('Selected model index: ' + str(np.argmax(np.mean(f1 , axis = 0))))\n",
    "\n",
    "print('Mean recall: ' + str(np.mean(recall , axis = 0)))\n",
    "print('Selected model index: ' + str(np.argmax(np.mean(recall , axis = 0))))\n",
    "\n",
    "print('Mean precision: ' + str(np.mean(precision , axis = 0)))\n",
    "print('Selected model index: ' + str(np.argmax(np.mean(precision , axis = 0))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Tree Tuning\n",
    "\n",
    "\n",
    "\n",
    "The built-in decision tree algorithm you are using has several parameters which you can tune (e.g., *max_depth* and *min_samples_leaf*). Use 5-fold cross-validation (e.g., reuse the code of task 2.4 and adapt the code for two parameters), show how the choice of these parameters affects performance. \n",
    "\n",
    "\n",
    "#### Tips: \n",
    "Make a decision on the range of values that you would try for the two parameters and discuss your choice in the experiment setup section.\n",
    "\n",
    "Here is a guide that helps you to build the experiment.\n",
    "First, reuse the code of task 2.4 and show how max_depth affects train and **validation accuracy**. On a single axis, plot train and **validation accuracy** as a function of max_depth. Use a red line to show validation accuracy and a blue line to show train accuracy. (Do not use your (held-out) **test data** yet). \n",
    "\n",
    "Second, show how validation accuracy relates to both max_depth and min_samples_leaf. Specifically, create a 3-D plot where the x-axis is max_depth, the y-axis is min_samples_leaf, and the z-axis shows accuracy. What combination of max_depth and min-samples_leaf achieves the highest F1 score? How sensitive are the results to these two parameters? \n",
    "\n",
    "Finally, select the best-performing decision tree (i.e., the one that achieved the highest cross-validated performance) and report the performance of the fitted model on the held-out test data -- how does it compare to the cross-validated F1 score?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: create 5-fold cross-validation\n",
    "\n",
    "# TODO: set the search space of the parameters\n",
    "\n",
    "# TODO: learn an optimal decision tree model\n",
    "\n",
    "# TODO: create 2D (or 3D) plot that shows how the selected parameters affect the performance. \n",
    "\n",
    "# TODO: compute the performance of the model on your held-out test data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INITIATE SCORING METRICS FOR ALL CLASSIFIERS #\n",
    "from sklearn.metrics import make_scorer, f1_score\n",
    "\n",
    "# Create scoring metrics and set zero_division parameter to 1 to avoid division by zero errors\n",
    "precision_scorer = make_scorer(precision_score, zero_division=1)\n",
    "recall_scorer = make_scorer(recall_score, zero_division=1)\n",
    "f1_scorer = make_scorer(f1_score, zero_division=1)\n",
    "\n",
    "# Create scoring dictionary for GridSearchCV\n",
    "scoring_dict = {'accuracy': 'accuracy', 'precision': precision_scorer, 'recall': recall_scorer, 'f1_score': f1_scorer}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRIDSEARCHCV FOR HYPERPARAMETER TUNING FOR DECISION TREE CLASSIFIER #\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import tree\n",
    "\n",
    "# Create unoptimized decision tree classifier object\n",
    "dtc_unoptimized = tree.DecisionTreeClassifier(random_state=0)\n",
    "\n",
    "# TODO: create 5-fold cross-validation\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=0)\n",
    "\n",
    "# TODO: set the search space of the parameters (by creating a parameter grid dictionary)\n",
    "dtc_param_grid_dict = { \n",
    "    'max_depth': [2, 3, 4, 5, 6, 7, 8, 9],\n",
    "    'min_samples_leaf': [1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
    "}\n",
    "\n",
    "# TODO: learn an optimal decision tree model\n",
    "dtc = tree.DecisionTreeClassifier(random_state=0)\n",
    "dtc_optimized = GridSearchCV(dtc, dtc_param_grid_dict, scoring=scoring_dict, cv=kf, return_train_score=False, refit='f1_score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the unoptimized decision tree classifier\n",
    "dtc_unoptimized.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the grid search to the data\n",
    "dtc_optimized.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# create 5-fold cross-validation\n",
    "nk = 5\n",
    "kf = KFold(n_splits=nk, random_state=0, shuffle=True)\n",
    "\n",
    "# Search the parameter among the following\n",
    "param_values = np.arange(2, 10)\n",
    "\n",
    "# init acc\n",
    "acc_train = np.zeros((nk , 8))\n",
    "acc_val = np.zeros((nk , 8))\n",
    "\n",
    "# init f1\n",
    "f1_train = np.zeros((nk , 8))\n",
    "f1_val = np.zeros((nk , 8))\n",
    "\n",
    "i = 0\n",
    "for train_index , val_index in kf.split(X):\n",
    "    X_t, X_val = X.iloc[train_index], X.iloc[val_index]\n",
    "    y_t, y_val = y.iloc[train_index], y.iloc[val_index]\n",
    "    j = 0\n",
    "    for c in param_values:\n",
    "        dt = tree.DecisionTreeClassifier(max_depth = c, random_state=0)\n",
    "        dt.fit(X_t, y_t)\n",
    "        yhat = dt.predict(X_t)\n",
    "        yhat2 = dt.predict(X_val)\n",
    "        acc_train[i][j] = accuracy_score(yhat , y_t)\n",
    "        f1_train[i][j] = f1_score(yhat , y_t)\n",
    "        acc_val[i][j] = accuracy_score(yhat2 , y_val)\n",
    "        f1_val[i][j] = f1_score(yhat2 , y_val)\n",
    "        j = j + 1\n",
    "    i = i + 1\n",
    "    \n",
    "# Store all mean accuracies of model on training data and validation data\n",
    "mean_train_acc = np.mean(acc_train , axis = 0)\n",
    "mean_val_acc = np.mean(acc_val , axis = 0)\n",
    "mean_acc = list(mean_train_acc) + list(mean_val_acc)\n",
    "\n",
    "# Prepare df for plotting\n",
    "groups = ['Training Data'] * len(mean_train_acc) + ['Validation Data'] * len(mean_val_acc)\n",
    "param_values = list(param_values) + list(param_values)\n",
    "\n",
    "# Create dataframe with columns \"accuracy\" and \"max_depth\" and \"group\"\n",
    "df_acc_comp = pd.DataFrame({'accuracy': mean_acc, 'max_depth': param_values, 'Group': groups})\n",
    "\n",
    "# Draw line chart\n",
    "fig = px.line(df_acc_comp, x=\"max_depth\", y=\"accuracy\", color='Group')\n",
    "\n",
    "# Edit x and y axes labels\n",
    "fig.update_layout(xaxis_title='Max Depth', yaxis_title='Accuracy')\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: create 2D (or 3D) plot that shows how the selected parameters affect the performance. \n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a dataframe from the grid search results\n",
    "dtc_results = pd.DataFrame(dtc_optimized.cv_results_)\n",
    "\n",
    "# Multiply all values in the 'mean_test_accuracy' column by 100\n",
    "dtc_results['mean_test_accuracy'] = dtc_results['mean_test_accuracy'] * 100\n",
    "\n",
    "# Create a pivot table from the dataframe\n",
    "dtc_pivot_table = dtc_results.pivot(index='param_max_depth', columns='param_min_samples_leaf', values='mean_test_accuracy')\n",
    "\n",
    "# Create a heatmap from the pivot table\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "sns.heatmap(dtc_pivot_table, annot=True, fmt='.3f', cmap='Blues', ax=ax)\n",
    "ax.set_title('Decision Tree Classifier Validation Accuracy')\n",
    "ax.set_xlabel('Min Samples Leaf')\n",
    "ax.set_ylabel('Max Depth')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pivot table from the dataframe\n",
    "dtc_pivot_table = dtc_results.pivot(index='param_max_depth', columns='param_min_samples_leaf', values='mean_test_f1_score')\n",
    "\n",
    "# Create a heatmap from the pivot table\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "sns.heatmap(dtc_pivot_table, annot=True, fmt='.3f', cmap='Blues', ax=ax)\n",
    "ax.set_title('Decision Tree Classifier Validation F1 Score')\n",
    "ax.set_xlabel('Min Samples Leaf')\n",
    "ax.set_ylabel('Max Depth')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create a figure that shows the min samples leaf on the x axis and the accuracy on the y axis\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "sns.lineplot(data=dtc_results, x='param_min_samples_leaf', y='mean_test_accuracy', hue='param_max_depth', ax=ax)\n",
    "ax.set_title('Decision Tree Classifier Accuracy')\n",
    "ax.set_xlabel('Min Samples Leaf')\n",
    "ax.set_ylabel('Accuracy')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "sns.lineplot(data=dtc_results, x='param_max_depth', y='mean_test_accuracy', hue='param_min_samples_leaf', ax=ax)\n",
    "ax.set_title('Decision Tree Classifier Accuracy')\n",
    "ax.set_xlabel('Max Depth')\n",
    "ax.set_ylabel('Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on test data\n",
    "y_pred_dtc_optimized = dtc_optimized.predict(X_test)\n",
    "\n",
    "# Save scores in variables\n",
    "accuracy_score_test = accuracy_score(y_test, y_pred_dtc_optimized)\n",
    "recall_score_test = recall_score(y_test, y_pred_dtc_optimized)\n",
    "precision_score_test = precision_score(y_test, y_pred_dtc_optimized)\n",
    "f1_score_test = f1_score(y_test, y_pred_dtc_optimized)\n",
    "dtc_on_test = [accuracy_score_test, recall_score_test, precision_score_test, f1_score_test]\n",
    "\n",
    "# Store gridserach results in dataframe\n",
    "dtc_on_train = pd.DataFrame(dtc_optimized.cv_results_)\n",
    "\n",
    "# Only keep row where rank_test_f1_score is 1\n",
    "dtc_on_train = dtc_on_train[dtc_on_train['rank_test_f1_score'] == 1]\n",
    "dtc_on_train = dtc_on_train[['mean_test_accuracy', 'mean_test_recall', 'mean_test_precision', 'mean_test_f1_score']].values.tolist()[0]\n",
    "\n",
    "# Create a barchart with all the models and their accuracies\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Bar(name=f\"Validation Data\", x=['Accuracy'], y=[dtc_on_train[0]*100], marker_color='#003f5c'))\n",
    "fig.add_trace(go.Bar(name=f\"Test Data\", x=['Accuracy'], y=[dtc_on_test[0]*100], marker_color='#58508d'))\n",
    "fig.update_layout(title='Accuracy score comparison between cross-validation and test data on DT Classifier', yaxis_title='F1 score', yaxis_range=[90, 92])\n",
    "fig.show()\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Bar(name=f\"Validation Data\", x=['Recall', 'Precision', 'F1'], y=dtc_on_train[1:], marker_color='#003f5c'))\n",
    "fig.add_trace(go.Bar(name=f\"Test Data\", x=['Recall', 'Precision', 'F1'], y=dtc_on_test[1:], marker_color='#58508d'))\n",
    "fig.update_layout(title='F1 score comparison between cross-validation and test data on DT Classifier', yaxis_title='F1 score', yaxis_range=[0.45, 0.65])\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put the results in a dataframe\n",
    "dtc_results = pd.DataFrame(dtc_optimized.cv_results_)\n",
    "\n",
    "# Only keep the rows where rank_test_accuracy is 1 or rank_test_f1_score is 1 or rank_test_precision is 1 or rank_test_recall is 1\n",
    "dtc_results = dtc_results[(dtc_results['rank_test_accuracy'] == 1) | (dtc_results['rank_test_recall'] == 1) | (dtc_results['rank_test_precision'] == 1) | (dtc_results['rank_test_f1_score'] == 1)]\n",
    "\n",
    "# Print the params column where rank_test_accuracy is 1 as string\n",
    "params_ofbest_accuracy = str(dtc_results[dtc_results['rank_test_accuracy'] == 1]['params'].values[0])\n",
    "\n",
    "# Assign the mean_test_accuracy column where rank_test_accuracy is 1 as decimal\n",
    "accuracy_ofbest_accuracy = dtc_results[dtc_results['rank_test_accuracy'] == 1]['mean_test_accuracy'].values[0]\n",
    "recall_ofbest_accuracy = dtc_results[dtc_results['rank_test_accuracy'] == 1]['mean_test_recall'].values[0]\n",
    "precision_ofbest_accuracy = dtc_results[dtc_results['rank_test_accuracy'] == 1]['mean_test_precision'].values[0]\n",
    "f1_ofbest_accuracy = dtc_results[dtc_results['rank_test_accuracy'] == 1]['mean_test_f1_score'].values[0]\n",
    "\n",
    "# Assign the params column where rank_test_f1_score is 1 as string\n",
    "params_ofbest_f1 = str(dtc_results[dtc_results['rank_test_f1_score'] == 1]['params'].values[0])\n",
    "\n",
    "# Assign the mean_test_f1_score column where rank_test_f1_score is 1 as decimal\n",
    "accuracy_ofbest_f1 = dtc_results[dtc_results['rank_test_f1_score'] == 1]['mean_test_accuracy'].values[0]\n",
    "recall_ofbest_f1 = dtc_results[dtc_results['rank_test_f1_score'] == 1]['mean_test_recall'].values[0]\n",
    "precision_ofbest_f1 = dtc_results[dtc_results['rank_test_f1_score'] == 1]['mean_test_precision'].values[0]\n",
    "f1_ofbest_f1 = dtc_results[dtc_results['rank_test_f1_score'] == 1]['mean_test_f1_score'].values[0]\n",
    "\n",
    "# Assign the params column where rank_test_precision is 1 as string\n",
    "params_ofbest_precision = str(dtc_results[dtc_results['rank_test_precision'] == 1]['params'].values[0])\n",
    "\n",
    "# Assign the mean_test_precision column where rank_test_precision is 1 as decimal\n",
    "accuracy_ofbest_precision = dtc_results[dtc_results['rank_test_precision'] == 1]['mean_test_accuracy'].values[0]\n",
    "recall_ofbest_precision = dtc_results[dtc_results['rank_test_precision'] == 1]['mean_test_recall'].values[0]\n",
    "precision_ofbest_precision = dtc_results[dtc_results['rank_test_precision'] == 1]['mean_test_precision'].values[0]\n",
    "f1_ofbest_precision = dtc_results[dtc_results['rank_test_precision'] == 1]['mean_test_f1_score'].values[0]\n",
    "\n",
    "# Assign the params column where rank_test_recall is 1 as string\n",
    "params_ofbest_recall = str(dtc_results[dtc_results['rank_test_recall'] == 1]['params'].values[0])\n",
    "\n",
    "# Assign the mean_test_recall column where rank_test_recall is 1 as decimal\n",
    "accuracy_ofbest_recall = dtc_results[dtc_results['rank_test_recall'] == 1]['mean_test_accuracy'].values[0]\n",
    "recall_ofbest_recall = dtc_results[dtc_results['rank_test_recall'] == 1]['mean_test_recall'].values[0]\n",
    "precision_ofbest_recall = dtc_results[dtc_results['rank_test_recall'] == 1]['mean_test_precision'].values[0]\n",
    "f1_ofbest_recall = dtc_results[dtc_results['rank_test_recall'] == 1]['mean_test_f1_score'].values[0]\n",
    "\n",
    "layout = go.Layout(\n",
    "    #legend=dict(x=0, y=-0.1, orientation='h'),\n",
    ")\n",
    "\n",
    "# Create a barchart with all the models and their accuracies\n",
    "fig = go.Figure(layout=layout)\n",
    "fig.add_trace(go.Bar(name=f\"Best Accuracy Model: {params_ofbest_accuracy}\", x=['Accuracy'], y=[accuracy_ofbest_accuracy*100], marker_color='#003f5c'))\n",
    "fig.add_trace(go.Bar(name=f\"Best Recall Model: {params_ofbest_recall}\", x=['Accuracy'], y=[accuracy_ofbest_recall*100], marker_color='#58508d'))\n",
    "fig.add_trace(go.Bar(name=f\"Best Precision Model: {params_ofbest_precision}\", x=['Accuracy'], y=[accuracy_ofbest_precision*100], marker_color='#bc5090'))\n",
    "fig.add_trace(go.Bar(name=f\"Best F1 Score Model: {params_ofbest_f1}\", x=['Accuracy'], y=[accuracy_ofbest_f1*100], marker_color='#ff6361'))\n",
    "fig.update_layout(title='Accuracy score of DT models with best performance per scoring metric', yaxis_title='Accuracy in %', yaxis_range=[90, 92])\n",
    "fig.show()\n",
    "\n",
    "fig = go.Figure(layout=layout)\n",
    "fig.add_trace(go.Bar(name=f\"Best Accuracy Model: {params_ofbest_accuracy}\", x=[\"Recall\", \"Precision\", \"F1\"], y=[recall_ofbest_accuracy, precision_ofbest_accuracy, f1_ofbest_accuracy], marker_color='#003f5c'))\n",
    "fig.add_trace(go.Bar(name=f\"Best Recall Model: {params_ofbest_recall}\", x=[\"Recall\", \"Precision\", \"F1\"], y=[recall_ofbest_recall, precision_ofbest_recall, f1_ofbest_recall], marker_color='#58508d'))\n",
    "fig.add_trace(go.Bar(name=f\"Best Precision Model: {params_ofbest_precision}\", x=[\"Recall\", \"Precision\", \"F1\"], y=[recall_ofbest_precision, precision_ofbest_precision, f1_ofbest_precision], marker_color='#bc5090'))\n",
    "fig.add_trace(go.Bar(name=f\"Best F1 Score Model: {params_ofbest_f1}\", x=[\"Recall\", \"Precision\", \"F1\"], y=[recall_ofbest_f1, precision_ofbest_f1, f1_ofbest_f1], marker_color='#ff6361'))\n",
    "fig.update_layout(title='Recall, precision and F1 score for DT models with best performance per scoring metric', yaxis_title='Score', yaxis_range=[0.3, 0.7])\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3. Random Forest\n",
    "### - Outlier Detection as a Supervised Classification\n",
    "\n",
    "Now use a [Random Forest](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html) to predict the labels for the data set. \n",
    "\n",
    "i) use the default values for the parameters to get a RF model running. \n",
    "\n",
    "ii) use 5-fold cross-validation to determine a possibly better choice for the parameter *n_estimators* and *max_features*\n",
    "    \n",
    "iii) select the best-performing decision tree (i.e., the one that achieved the highest cross-validated performance) and report the performance of the fitted model on the held-out test data ?\n",
    "\n",
    "In the report, reflect on how does the test performance of RF compare to the decision tree performance? \n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# TODO: Ccreate 5-fold cross-validation\n",
    "\n",
    "# TODO: set the search space of the parameters\n",
    "\n",
    "# TODO: learn an optimal random forest model\n",
    "\n",
    "# TODO: compute the performance of the model on your held-out test data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRIDSEARCHCV FOR HYPERPARAMETER TUNING FOR RANDOM FOREST CLASSIFIER #\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Create unoptimized random forest classifier object for comparison\n",
    "rfc_unoptimized = RandomForestClassifier(random_state=0)\n",
    "\n",
    "# TODO: create 5-fold cross-validation\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=0)\n",
    "\n",
    "# TODO: set the search space of the parameters\n",
    "rdf_param_grid_dict = {\n",
    "    'n_estimators': [100, 150],\n",
    "    'max_features': ['sqrt', 'log2', None]\n",
    "}\n",
    "\n",
    "# TODO: learn an optimal random forest model\n",
    "rfc = RandomForestClassifier(random_state=0)\n",
    "rfc_optimized = GridSearchCV(rfc, rdf_param_grid_dict, scoring=scoring_dict, cv=kf, return_train_score=False, refit='f1_score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the unoptimized random forest classifier to the training data\n",
    "rfc_unoptimized.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the optimized random forest grid search classifier to the training data\n",
    "rfc_optimized.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on test data\n",
    "y_pred_rfc_optimized = rfc_optimized.predict(X_test)\n",
    "\n",
    "# Save scores in variables\n",
    "accuracy_score_test = accuracy_score(y_test, y_pred_rfc_optimized)\n",
    "recall_score_test = recall_score(y_test, y_pred_rfc_optimized)\n",
    "precision_score_test = precision_score(y_test, y_pred_rfc_optimized)\n",
    "f1_score_test = f1_score(y_test, y_pred_rfc_optimized)\n",
    "rfc_on_test = [accuracy_score_test, recall_score_test, precision_score_test, f1_score_test]\n",
    "\n",
    "# Store gridserach results in dataframe\n",
    "rfc_on_train = pd.DataFrame(rfc_optimized.cv_results_)\n",
    "\n",
    "# Only keep row where rank_test_f1_score is 1\n",
    "rfc_on_train = rfc_on_train[rfc_on_train['rank_test_f1_score'] == 1]\n",
    "rfc_on_train = rfc_on_train[['mean_test_accuracy', 'mean_test_recall', 'mean_test_precision', 'mean_test_f1_score']].values.tolist()[0]\n",
    "\n",
    "# Create a barchart with all the models and their accuracies\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Bar(name=f\"Validation Data\", x=['Accuracy'], y=[rfc_on_train[0]*100], marker_color='#003f5c'))\n",
    "fig.add_trace(go.Bar(name=f\"Test Data\", x=['Accuracy'], y=[rfc_on_test[0]*100], marker_color='#58508d'))\n",
    "fig.update_layout(title='Accuracy score comparison between cross-validation and test data for RF', yaxis_title='F1 score', yaxis_range=[91, 93])\n",
    "fig.show()\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Bar(name=f\"Validation Data\", x=['Recall', 'Precision', 'F1'], y=rfc_on_train[1:], marker_color='#003f5c'))\n",
    "fig.add_trace(go.Bar(name=f\"Test Data\", x=['Recall', 'Precision', 'F1'], y=rfc_on_test[1:], marker_color='#58508d'))\n",
    "fig.update_layout(title='F1 score comparison between cross-validation and test data for RF', yaxis_title='F1 score', yaxis_range=[0.2, 0.7])\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'go' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Jesse\\Desktop\\Coding Projects\\GitHub\\apml\\Assignment-1.ipynb Cell 46\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Jesse/Desktop/Coding%20Projects/GitHub/apml/Assignment-1.ipynb#Y125sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Create a barchart comparing scoring metrics of best f1 performing DT and RF model\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Jesse/Desktop/Coding%20Projects/GitHub/apml/Assignment-1.ipynb#Y125sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m fig \u001b[39m=\u001b[39m go\u001b[39m.\u001b[39mFigure()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Jesse/Desktop/Coding%20Projects/GitHub/apml/Assignment-1.ipynb#Y125sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m fig\u001b[39m.\u001b[39madd_trace(go\u001b[39m.\u001b[39mBar(name\u001b[39m=\u001b[39m\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mDT\u001b[39m\u001b[39m\"\u001b[39m, x\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39mAccuracy\u001b[39m\u001b[39m'\u001b[39m], y\u001b[39m=\u001b[39m[dtc_on_test[\u001b[39m0\u001b[39m]\u001b[39m*\u001b[39m\u001b[39m100\u001b[39m], marker_color\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m#003f5c\u001b[39m\u001b[39m'\u001b[39m))\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Jesse/Desktop/Coding%20Projects/GitHub/apml/Assignment-1.ipynb#Y125sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m fig\u001b[39m.\u001b[39madd_trace(go\u001b[39m.\u001b[39mBar(name\u001b[39m=\u001b[39m\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mRF\u001b[39m\u001b[39m\"\u001b[39m, x\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39mAccuracy\u001b[39m\u001b[39m'\u001b[39m], y\u001b[39m=\u001b[39m[rfc_on_test[\u001b[39m0\u001b[39m]\u001b[39m*\u001b[39m\u001b[39m100\u001b[39m], marker_color\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m#58508d\u001b[39m\u001b[39m'\u001b[39m))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'go' is not defined"
     ]
    }
   ],
   "source": [
    "# Create a barchart comparing scoring metrics of best f1 performing DT and RF model\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Bar(name=f\"DT\", x=['Accuracy'], y=[dtc_on_test[0]*100], marker_color='#003f5c'))\n",
    "fig.add_trace(go.Bar(name=f\"RF\", x=['Accuracy'], y=[rfc_on_test[0]*100], marker_color='#58508d'))\n",
    "fig.update_layout(title='Accuracy comparison between best performing DT and RF model based on F1 Score', yaxis_title='Accuracy in %', yaxis_range=[91, 93])\n",
    "fig.show()\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Bar(name=f\"DT\", x=['Recall', 'Precision', 'F1'], y=dtc_on_test[1:], marker_color='#003f5c'))\n",
    "fig.add_trace(go.Bar(name=f\"RF\", x=['Recall', 'Precision', 'F1'], y=rfc_on_test[1:], marker_color='#58508d'))\n",
    "fig.update_layout(title='Recall, precision and F1 score comparison between best performing DT and RF model based on F1 Score', yaxis_title='Score', yaxis_range=[0.4, 0.7])\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4. Isolation Forest\n",
    "\n",
    "### 4.1 Apply Isolation Forest\n",
    "### - Outlier Detection as an Unsupervised Classification\n",
    "\n",
    "Use the [Isolation Forest Classifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.IsolationForest.html) to detect potential outliers in the data set. \n",
    "\n",
    "Select two parameters that you would like to investigate (for example, contamination, max_depth, n_estimators, max_samples). For each configuration: \n",
    "\n",
    "i) Apply Isolation Forest on the full data set (without using the labels Y)\n",
    "\n",
    "\n",
    "ii) Use the labels to compute the accuracy, recall, precision, and F1-score on the full data set (using the labels). \n",
    "\n",
    "\n",
    "Compare the performance of Isolation Forest of different configurations. \n",
    "\n",
    "\n",
    "#### Tips:\n",
    "\n",
    "- Note that the fit(X) function of the Isolation Forest does not use the labels. \n",
    "\n",
    "\n",
    "- **Look carefully at the values that an Isolation Forest classifier returns. Which value represents the outlier class? Be aware that you need to implement a mapping function f(x) that remaps -1 to 1 and 1 to 0, in order to transform the predictions such that the semantics are consistant with the previous classification algorithms.**\n",
    "\n",
    "\n",
    "- Create 2D (or 3D) plots to visualize your results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "# TODO: set the search space of the parameters\n",
    "\n",
    "# TODO: apply the configured Isolation Forest model on the test set. \n",
    "\n",
    "# TODO: compute the performance of the model\n",
    "\n",
    "# TODO: return the optimal Isolation Forest model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_parameter_values(results, parameter_name):\n",
    "    return [result['params'][parameter_name] for result in results]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(parameter_values, accuracy_values, parameter_name):\n",
    "    plt.scatter(parameter_values, accuracy_values)\n",
    "    plt.xlabel(parameter_name.capitalize())\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Isolation Forest Performance')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.metrics import accuracy_score, recall_score, precaision_score, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "param_grid = {\n",
    "    'random_state': [42],\n",
    "    'contamination': [0.01, 0.05, 0.1, 0.15],\n",
    "    'max_samples': [50, 100, 200],\n",
    "    'max_features': [0.5, 0.7, 1.0],\n",
    "    'n_estimators': [50, 100, 200],\n",
    "}\n",
    "\n",
    "results = []\n",
    "\n",
    "for params in ParameterGrid(param_grid):\n",
    "    iforest = IsolationForest(**params)\n",
    "    iforest.fit(X_train)\n",
    "\n",
    "    predictions = iforest.predict(X_test)\n",
    "    mapped_predictions = [1 if p == -1 else 0 for p in predictions]\n",
    "\n",
    "    accuracy = accuracy_score(y_test, mapped_predictions)\n",
    "    recall = recall_score(y_test, mapped_predictions)\n",
    "    precision = precision_score(y_test, mapped_predictions)\n",
    "    f1 = f1_score(y_test, mapped_predictions)\n",
    "\n",
    "    results.append({\n",
    "        'params': params,\n",
    "        'accuracy': accuracy,\n",
    "        'recall': recall,\n",
    "        'precision': precision,\n",
    "        'f1': f1,\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for result in results:\n",
    "    print(\"Parameters:\", result['params'])\n",
    "    print(\"Accuracy:\", result['accuracy'])\n",
    "    print(\"Recall:\", result['recall'])\n",
    "    print(\"Precision:\", result['precision'])\n",
    "    print(\"F1 Score:\", result['f1'])\n",
    "    print(\"\\n\")\n",
    "\n",
    "\n",
    "contamination_values = get_parameter_values(results, 'contamination')\n",
    "max_samples_values = get_parameter_values(results, 'max_samples')\n",
    "n_estimators_values = get_parameter_values(results, 'n_estimators')\n",
    "max_features_values = get_parameter_values(results, 'max_features')\n",
    "accuracy_values = [result['accuracy'] for result in results]\n",
    "\n",
    "plot_results(contamination_values, accuracy_values, 'contamination')\n",
    "plot_results(max_samples_values, accuracy_values, 'max_samples')\n",
    "plot_results(n_estimators_values, accuracy_values, 'n_estimators')\n",
    "plot_results(max_features_values, accuracy_values, 'max_features')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Outlier Detection - Analyze Outliers\n",
    "\n",
    "Taking the best performing Isolation Forest model from Task 4.1, enrich the data set with the predicted labels (or scores) by the model. \n",
    "\n",
    "Perform one or two analyses to show the characteristics of the outliers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: enrich the data with the anomaly scores assigned by the optimal model. \n",
    "\n",
    "# TODO: Perform one or two analyses to show the characteristics of the outliers. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enrich the data with the anomaly scores assigned by the optimal model. \n",
    "best_result = max(results, key=lambda x: x['f1'])\n",
    "best_params = best_result['params']\n",
    "\n",
    "best_iforest = IsolationForest(**best_params)\n",
    "best_iforest.fit(X_train)\n",
    "\n",
    "enriched_data = X_test.copy()\n",
    "enriched_data['isolation_forest_score'] = best_iforest.decision_function(X_test)\n",
    "\n",
    "# Perform one or two analyses to show the characteristics of the outliers. \n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(enriched_data.loc[y_test == 0, 'isolation_forest_score'], bins=50, label='Normal', alpha=0.5)\n",
    "plt.hist(enriched_data.loc[y_test == 1, 'isolation_forest_score'], bins=50, label='Outlier', alpha=0.5)\n",
    "plt.xlabel('Isolation Forest Score')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Isolation Forest Scores for Normal and Outlier Instances')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5. Report your results and discuss your findings\n",
    "\n",
    "### 5.1 Compare the performances \n",
    "\n",
    "By now, you have applied three algorithms with different parameters on the data set. For each algorithm, you have create tables or figures which you can add to your report. Discuss the results and their optimal performance. \n",
    "\n",
    "Create an overview table or figure that show the optimal performance of each algorithm on the data set, for example see the table here below. \n",
    "\n",
    "Discuss your findings in the report and reflect on the following questions:\n",
    "- According to the performance results, which one is the optimal model? \n",
    "- How large is the difference between the accuracy score and the F1 score for each model? What caused the difference?\n",
    "- Which of performance measures (the accuracy score, recall, precision, or F1-score) would you use for comparing the model performance? Why?\n",
    "- You are comparing the performance of supervised algorithms (DT and RF) with an unsupervised algorithm (Isolation Forest). Is this a fair comparison? Motivate your answer. \n",
    "\n",
    "\n",
    "\n",
    "| Model | Validation Accuracy  | Test Accuracy |  Validation Recall  |  Test Recall  | Validation F1 | Test F1 |... |\n",
    "|------|------|------|------|------|------|------|-----|\n",
    "|   Decision Tree        |  |  | | | | |\n",
    "|   Random Forest  |  |  | || | |\n",
    "|   Isolation Forest        |  |  | || | |\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Analyze and discuss the results\n",
    "\n",
    "For each optimal model, enrich your test set by adding the predicted labels by this model to the test set. Can you think of an analysis that gives insights into when the model performs poorly?\n",
    "\n",
    "Discuss the analysis and insights in the report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus Tasks \n",
    "\n",
    "We would like to challenge you with the following bonus tasks. For each task that is successfully completed, you may obtain max. 0.5 extra point. \n",
    "\n",
    "### Bonus Task 1\n",
    "\n",
    "Implement another outlier detection algorithm (for example, LOF, OC-SVM) or design your own outlier detection algorithm that achieves a better F1 score. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus Task 2 \n",
    "\n",
    "Implement techniques (e.g., preprocessing, feature engineering, sampling) that help improve the F1 scores of existing models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction Challenge \n",
    "\n",
    "- Import the independent test set without labels, apply your best performing model on this test set. \n",
    "\n",
    "- Enrich the test set with the predicted labels (**name this column 'predictedClass'**) \n",
    "\n",
    "- Export both the model as pkl file and the enriched test data set as a csv file. \n",
    "\n",
    "- The top three teams that have achieved the best accuracy score wins max. 0.3 bonus points.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# import data\n",
    "data_challenge = pd.read_csv('./dataBank-new_test_nolabel.csv', sep=',')\n",
    "X_new = data_challenge[features]\n",
    "print(X_new.describe())\n",
    "\n",
    "# TODO: assign optimal model \n",
    "optimal_model = ...\n",
    "\n",
    "yhat = optimal_model.predict(X_new)\n",
    "\n",
    "# TODO: enrich the data with the predicted labels by adding the column 'predictedClass'\n",
    "\n",
    "\n",
    "# TODO: export the enriched data to disk\n",
    "\n",
    "\n",
    "# export the model to disk\n",
    "modelfilename = 'Team_x_optimal_model.sav'\n",
    "pickle.dump(optimal_model, open(modelfilename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
